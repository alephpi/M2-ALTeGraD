{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><h2>ALTeGraD 2022<br>Lab Session 3: NLP Frameworks</h2> 15 / 11 / 2022<br> M. Kamal Eddine, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> Sicheng Mao\n",
        "\n",
        "</center>\n",
        "\n",
        "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers to pretrain and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative.\n",
        "\n",
        "In the first part of this lab, you will finetune the given model on model on CLS_Books dataset using <b>Fairseq</b> by following these steps:<br>\n",
        "\n",
        " 1- <b>Tokenize the reviews</b> (Train, Valid and Test) using trained sentencepiece tokenizer provided alongside the pretrained model.[using sentencepiece library and setting the parameter <b>out_type=str</b> in the encode function].<br>\n",
        " 2- <b>Binarize the tokenized reviews and their labels</b> using the preprocess python script provided in Fairseq.<br>\n",
        " 3- <b>Fintune the pretrained $RoBERTa_{small}^{fr}$ model</b> using the train python script provided in Fairseq.<br>\n",
        " \n",
        " Finally, you will finish the first part by training a random $RoBERTa_{small}^{fr}$ model on the CLS_Books dataset and compare the results against the pretrained model while <b>visualizing the accuracies on tensorboard</b>.\n",
        "\n",
        " In the second part of this lab, you will use <b>HuggingFace's transformers</b> library to perform the finrtuning done previously with Fairseq.\n",
        "\n"
      ],
      "metadata": {
        "id": "DsD-LMKT7XMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Part 1: Fairseq</b>"
      ],
      "metadata": {
        "id": "H40TxVIvEWyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Preparing the environment and installing libraries, model and data</b>\n",
        "\n",
        "In this section, we will setup the environment on Google Colab (first cell), download the pretraind model (second cell) and the finetuning dataset (third cell). In case you are using your personal computer maket sure to:\n",
        "\n",
        "1- Use Ubuntu (or any similar linux distribution) or MacOS. <b> P.S. In case you have Windows, please use Google Colab. We won't respond to any question regarding errors on Windows. </b>\n",
        "\n",
        "2- <b>Use Anaconda</b> and create new environment if you already installed Fairseq since we will be using a slightly modified version of this library.\n",
        "\n",
        "3- <b>Do not run the following three cells</b>. Instead, use their content on your personal command line."
      ],
      "metadata": {
        "id": "GwN3KCm5Ec6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir altegrad.lab3 && cd altegrad.lab3 && mkdir libs \n",
        "%cd altegrad.lab3/libs\n",
        "!git clone https://github.com/hadi-abdine/fairseq\n",
        "!pip install git+https://github.com/hadi-abdine/fairseq\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install sentencepiece\n",
        "!pip install tensorboardX\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "6cGqZ9ZB84Cd",
        "outputId": "d3a203ec-600d-4258-a49c-cba33a5fb2dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/altegrad.lab3/libs\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 19737, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 19737 (delta 0), reused 4 (delta 0), pack-reused 19730\u001b[K\n",
            "Receiving objects: 100% (19737/19737), 18.52 MiB | 22.58 MiB/s, done.\n",
            "Resolving deltas: 100% (14337/14337), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/hadi-abdine/fairseq\n",
            "  Cloning https://github.com/hadi-abdine/fairseq to /tmp/pip-req-build-plxcnykp\n",
            "  Running command git clone -q https://github.com/hadi-abdine/fairseq /tmp/pip-req-build-plxcnykp\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/ngoyal2707/Megatron-LM\n",
            "   * branch            adb23324c222aad0aad89308e70302d996a5eaeb -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.29.32)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2022.6.2)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (4.64.1)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.21.6)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.12.1+cu113)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.12.1+cu113)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (5.10.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.1)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (3.10.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp37-cp37m-linux_x86_64.whl size=15514509 sha256=c3aa2d4f86062bb5ccf05c3d624ea00932cff9cce8a2da88bae4037f4773a5d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-flzwmdt_/wheels/04/71/b7/a926c1b92d7336cc0fca23a8e6dfa4a6ac8d2c81b71f3dfa3d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=6e745c0bf9491965deb954a6f6f5bc448a32beb24b10725687544ee058501aa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.6.0 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.6.0 sacrebleu-2.3.1\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 116349, done.\u001b[K\n",
            "remote: Counting objects: 100% (457/457), done.\u001b[K\n",
            "remote: Compressing objects: 100% (294/294), done.\u001b[K\n",
            "remote: Total 116349 (delta 226), reused 262 (delta 132), pack-reused 115892\u001b[K\n",
            "Receiving objects: 100% (116349/116349), 109.95 MiB | 26.85 MiB/s, done.\n",
            "Resolving deltas: 100% (86559/86559), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-h42pqql1\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-h42pqql1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.25.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.25.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (3.0.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.25.0.dev0-py3-none-any.whl size=5734510 sha256=4ea4a2daf1aee63863243c1f48799b55426e164031c17a9258fa2d8c2d58e21c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n3beasyx/wheels/90/a5/44/6bcd83827c8a60628c5ad602f429cd5076bcce5f2a90054947\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.25.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.7.0-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 66.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.7.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from evaluate) (4.13.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (2.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from evaluate) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from evaluate) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.11.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from evaluate) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from evaluate) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (2022.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->evaluate) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->evaluate) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 8.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.19.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && mkdir models \n",
        "%cd ../models\n",
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%21267604&authkey=ANaaKIDigQPyJlM\" -O \"model_fairseq.zip\"\n",
        "!unzip model_fairseq.zip\n",
        "!rm model_fairseq.zip\n",
        "!rm -rf __MACOSX/"
      ],
      "metadata": {
        "id": "rxHffsPm-EqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8036fc6-af5f-4338-9a61-c515d1038e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/altegrad.lab3/models\n",
            "--2022-11-19 08:35:21--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%21267604&authkey=ANaaKIDigQPyJlM\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://kiebga.am.files.1drv.com/y4malmMYra-lF9upyMRpfHsc2ZuMglBMciBn3sSAxHdcYi2YFE2c99h5HCl_NCkSpDkccDRfndjnIaXrU86JZ_6QmTvTo72FSn4jdxkvEJuHeGKL6ixW50R8NmcOcm-S2NvKMQavaQePoqcgYBAxEb3fn-EzY-uU6CuCojaMLwXlTSQqWr_KrO3uqtwiaIR6kTmV23MnjHXgHXSxs_J9-0Qsg/RoBERTa_small_fr.zip?download&psid=1 [following]\n",
            "--2022-11-19 08:35:22--  https://kiebga.am.files.1drv.com/y4malmMYra-lF9upyMRpfHsc2ZuMglBMciBn3sSAxHdcYi2YFE2c99h5HCl_NCkSpDkccDRfndjnIaXrU86JZ_6QmTvTo72FSn4jdxkvEJuHeGKL6ixW50R8NmcOcm-S2NvKMQavaQePoqcgYBAxEb3fn-EzY-uU6CuCojaMLwXlTSQqWr_KrO3uqtwiaIR6kTmV23MnjHXgHXSxs_J9-0Qsg/RoBERTa_small_fr.zip?download&psid=1\n",
            "Resolving kiebga.am.files.1drv.com (kiebga.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to kiebga.am.files.1drv.com (kiebga.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264887103 (253M) [application/zip]\n",
            "Saving to: ‘model_fairseq.zip’\n",
            "\n",
            "model_fairseq.zip   100%[===================>] 252.62M  16.8MB/s    in 15s     \n",
            "\n",
            "2022-11-19 08:35:40 (16.4 MB/s) - ‘model_fairseq.zip’ saved [264887103/264887103]\n",
            "\n",
            "Archive:  model_fairseq.zip\n",
            "   creating: RoBERTa_small_fr/\n",
            "  inflating: RoBERTa_small_fr/model.pt  \n",
            "  inflating: __MACOSX/RoBERTa_small_fr/._model.pt  \n",
            "  inflating: RoBERTa_small_fr/sentencepiece.bpe.model  \n",
            "  inflating: RoBERTa_small_fr/dict.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && mkdir data\n",
        "%cd ../data\n",
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%21267606&authkey=AA8Td6LoeijplD4\" -O \"cls.books.zip\"\n",
        "!unzip cls.books.zip\n",
        "!rm cls.books.zip\n",
        "!rm -rf __MACOSX/\n",
        "%cd .."
      ],
      "metadata": {
        "id": "HfZ_znATNdya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4ec494-8f1c-4532-e016-6c414929e744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/altegrad.lab3/data\n",
            "--2022-11-19 08:35:44--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%21267606&authkey=AA8Td6LoeijplD4\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://yapqxq.am.files.1drv.com/y4mLVYhbQYCVWPyDq0Wy4qLFrsUx6Tk-AKlEadWVO7ywUfXrh78Hn0ZxkzlgdN2rzchbRTKcYpUSkU4BpSgS3WIO8GZVpeYPErBSGRXRa3T7Sv3BYTL2_op6TljUFBqHGISaPAkCEYetUphmb-Gz-MEBzfRBMV1FAGqMsuPAan5VBkQY7Laep8cie7UwSc0PMk1APF2Auc4npKuCXjtpPkcnQ/cls.books.zip?download&psid=1 [following]\n",
            "--2022-11-19 08:35:45--  https://yapqxq.am.files.1drv.com/y4mLVYhbQYCVWPyDq0Wy4qLFrsUx6Tk-AKlEadWVO7ywUfXrh78Hn0ZxkzlgdN2rzchbRTKcYpUSkU4BpSgS3WIO8GZVpeYPErBSGRXRa3T7Sv3BYTL2_op6TljUFBqHGISaPAkCEYetUphmb-Gz-MEBzfRBMV1FAGqMsuPAan5VBkQY7Laep8cie7UwSc0PMk1APF2Auc4npKuCXjtpPkcnQ/cls.books.zip?download&psid=1\n",
            "Resolving yapqxq.am.files.1drv.com (yapqxq.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to yapqxq.am.files.1drv.com (yapqxq.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955190 (933K) [application/zip]\n",
            "Saving to: ‘cls.books.zip’\n",
            "\n",
            "cls.books.zip       100%[===================>] 932.80K   866KB/s    in 1.1s    \n",
            "\n",
            "2022-11-19 08:35:47 (866 KB/s) - ‘cls.books.zip’ saved [955190/955190]\n",
            "\n",
            "Archive:  cls.books.zip\n",
            "   creating: cls.books/\n",
            "  inflating: cls.books/valid.label   \n",
            "  inflating: cls.books/test.review   \n",
            "  inflating: cls.books/valid.review  \n",
            "  inflating: cls.books/train.review  \n",
            "  inflating: cls.books/train.label   \n",
            "  inflating: cls.books/test.label    \n",
            "/content/altegrad.lab3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Number of parameters of the model</b>\n",
        "\n",
        "In this section you have to compute the number of parameters of $RoBERTa_{small}^{fr}$ using PyTorch. (<b>Hint:</b> you can check the architecture of the model using model['model'])"
      ],
      "metadata": {
        "id": "TvpyZEexOXHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = torch.load(\"models/RoBERTa_small_fr/model.pt\")"
      ],
      "metadata": {
        "id": "j7isz60LOwlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b004483b-5c28-4f3d-bb73-8e2fa325d901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg8UTrGiEjlc",
        "outputId": "3e874d18-51c7-436d-a1e8-df173ad18b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "type(model['model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb2la2INEFdM",
        "outputId": "cfce4ed7-5bb5-407a-ce02-5008f3fd9f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "collections.OrderedDict"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (key, value) in model['model'].items():\n",
        "  print(key, value.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4SXhLfJE29b",
        "outputId": "07c10f2f-d689-4827-f12e-9ca5ad91b6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder.sentence_encoder.version torch.Size([1])\n",
            "encoder.sentence_encoder.embed_tokens.weight torch.Size([32000, 512])\n",
            "encoder.sentence_encoder.embed_positions.weight torch.Size([258, 512])\n",
            "encoder.sentence_encoder.layernorm_embedding.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layernorm_embedding.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.k_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.v_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.q_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.out_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.self_attn_layer_norm.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.self_attn_layer_norm.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.fc1.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.fc1.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.fc2.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.fc2.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.final_layer_norm.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layers.0.final_layer_norm.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.k_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.v_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.q_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.out_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.self_attn_layer_norm.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.self_attn_layer_norm.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.fc1.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.fc1.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.fc2.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.fc2.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.final_layer_norm.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layers.1.final_layer_norm.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.k_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.v_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.q_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.out_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.self_attn_layer_norm.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.self_attn_layer_norm.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.fc1.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.fc1.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.fc2.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.fc2.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.final_layer_norm.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layers.2.final_layer_norm.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.k_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.k_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.v_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.v_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.q_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.q_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.out_proj.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.out_proj.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.self_attn_layer_norm.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.self_attn_layer_norm.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.fc1.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.fc1.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.fc2.weight torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.fc2.bias torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.final_layer_norm.weight torch.Size([512])\n",
            "encoder.sentence_encoder.layers.3.final_layer_norm.bias torch.Size([512])\n",
            "encoder.lm_head.weight torch.Size([32000, 512])\n",
            "encoder.lm_head.bias torch.Size([32000])\n",
            "encoder.lm_head.dense.weight torch.Size([512, 512])\n",
            "encoder.lm_head.dense.bias torch.Size([512])\n",
            "encoder.lm_head.layer_norm.weight torch.Size([512])\n",
            "encoder.lm_head.layer_norm.bias torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_parameters = 0\n",
        "layer_include = ['embed','layers']\n",
        "layer_exclude = ['bias', 'norm']\n",
        "for (key, value) in model['model'].items():\n",
        "  if any(word in key for word in layer_include) and not any(word in key for word in layer_exclude):\n",
        "    print(key, value.numel())\n",
        "    n_parameters += value.numel()\n",
        "print(n_parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "796fuQqRExWm",
        "outputId": "b8695655-b8ed-48d6-b006-2987cd87570c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder.sentence_encoder.embed_tokens.weight 16384000\n",
            "encoder.sentence_encoder.embed_positions.weight 132096\n",
            "encoder.sentence_encoder.layers.0.self_attn.k_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.0.self_attn.v_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.0.self_attn.q_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.0.self_attn.out_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.0.fc1.weight 262144\n",
            "encoder.sentence_encoder.layers.0.fc2.weight 262144\n",
            "encoder.sentence_encoder.layers.1.self_attn.k_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.1.self_attn.v_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.1.self_attn.q_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.1.self_attn.out_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.1.fc1.weight 262144\n",
            "encoder.sentence_encoder.layers.1.fc2.weight 262144\n",
            "encoder.sentence_encoder.layers.2.self_attn.k_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.2.self_attn.v_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.2.self_attn.q_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.2.self_attn.out_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.2.fc1.weight 262144\n",
            "encoder.sentence_encoder.layers.2.fc2.weight 262144\n",
            "encoder.sentence_encoder.layers.3.self_attn.k_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.3.self_attn.v_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.3.self_attn.q_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.3.self_attn.out_proj.weight 262144\n",
            "encoder.sentence_encoder.layers.3.fc1.weight 262144\n",
            "encoder.sentence_encoder.layers.3.fc2.weight 262144\n",
            "22807552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Tokenizing the reviews</b>\n",
        "\n",
        "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets. \n",
        "\n",
        "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize the three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews."
      ],
      "metadata": {
        "id": "gz8fnWOSI0eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "s = spm.SentencePieceProcessor(model_file='models/RoBERTa_small_fr/sentencepiece.bpe.model')"
      ],
      "metadata": {
        "id": "D-hOlotmOW7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPLITS=['train', 'test', 'valid']\n",
        "SENTS=\"review\"\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.'+SENTS, 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "        print(reviews[0])\n",
        "        reviews = [' '.join(s.encode(review, out_type = str)) for review in reviews]# tokenize data\n",
        "        \n",
        "        #It should look something like that\n",
        "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
        "        print(reviews[0]) \n",
        "    with open('data/cls.books/'+split+'.spm.'+SENTS, 'w') as f:\n",
        "        for review in reviews:\n",
        "          f.write(review+'\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz6FQh3hMohr",
        "outputId": "364b9b00-3266-4c3d-ca20-c5c76f9f8c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ce livre est tout simplement magique !  il vaut le detour suspense, humour, magie, tristesse, courage,sont parfaitement regroupés dans cet ouvrage ... Un vrai chef-d'oeuvre ! Bien que le sort de Harry soit prévisible, en raison de sa sortie quasi simultanée avec les deux tomes suivants ; ce livre est un régal que se soit pour les petits ou les grands !!! Johanne Kathleen Rowling a su alier plusieurs histoires qui paraissent totalement différentes mais avec une fin qui leur est communne et quasi introuvable jusqu'à la fin ! Elle à également su comment faire pour attirer une génération de non lecteurs avec ses livres \"énormes\" et sans images ! Entre nous il ya de quoi se demander qui le magicien Harry ou elle ? Bravo !!!\n",
            "\n",
            "▁Ce ▁livre ▁est ▁tout ▁simplement ▁mag ique ▁! ▁il ▁vaut ▁le ▁de tour ▁suspens e , ▁ humour , ▁magie , ▁triste sse , ▁courage , son t ▁parfaitement ▁re group és ▁dans ▁cet ▁ ouvrage ▁... ▁Un ▁vrai ▁chef - d ' oeuvre ▁! ▁Bien ▁que ▁le ▁sort ▁de ▁Harry ▁soit ▁pré visible , ▁en ▁raison ▁de ▁sa ▁sortie ▁quasi ▁simultan ée ▁avec ▁les ▁deux ▁tome s ▁suivant s ▁; ▁ce ▁livre ▁est ▁un ▁rég al ▁que ▁se ▁soit ▁pour ▁les ▁petits ▁ou ▁les ▁grands ▁!!! ▁Johann e ▁Kat hle en ▁Row ling ▁a ▁su ▁ali er ▁plusieurs ▁histoire s ▁qui ▁para issent ▁totalement ▁différentes ▁mais ▁avec ▁une ▁fin ▁qui ▁leur ▁est ▁commun ne ▁et ▁quasi ▁intro uv able ▁jusqu ' à ▁la ▁fin ▁! ▁Elle ▁à ▁également ▁su ▁comment ▁faire ▁pour ▁att ir er ▁une ▁génération ▁de ▁non ▁lecteur s ▁avec ▁ses ▁livres ▁\" é norm es \" ▁et ▁sans ▁images ▁! ▁Entre ▁nous ▁il ▁ya ▁de ▁quoi ▁se ▁demander ▁qui ▁le ▁magic ien ▁Harry ▁ou ▁elle ▁? ▁Bravo ▁!!!\n",
            "J'ai lu ce livre car dans ma ville, tout le monde s'en sert et le commande. C'est ma pharmacienne qui me l'a conseillé, elle a tellement maigri que je lui ai demandé ce qu'elle avait fait et au lieu de me vendre du perlimpinpin en gélules, elle m'a conseillé ce livre à 5 euros. Bien sur, il faut faire un effort pour perdre 25 kilos mais avec le livre, j'avais un compagnon de route. L'auteur a su me parler simplement avec des arguments très forts et surtout j'ai senti qu'il connaissait bien des cas comme le mien. Il y a dans son texte de l'expérience, de la simplicité et de la compassion pour ceux qui comme moi vivait avec tout ce poids qui me collait au corps sans jamais vouloir partir. Je ne crois pas qu'il existe un régime miracle qui surpasse les autres mais je crois vraiment qu'il y a des personnes qui savent parler aux autres et faire naitre des déclics. Je croyais être faible mais ce livre m'a rendu forte, je l'ai tellement annoté que j'en suis à mon troisième. Quand on est très grosse comme je l'ai été, les non-gros ne vous comprennent pas ou ont peur de vous froisser en vous en parlant, alors ce livre a été comme un compagnon-journal.  Je suis pédicure et je l'ai conseillé à tous mes clients gros dont je lis la souffrance sur les pieds déformés et gonflés. je rends aux autres le service que m'a rendu ma pharmacienne. Je le conseille à tous ceux qui souffrent car après avoir maigri c'est un tel bonheur que j'ai accepté de passer à la phase 3 de ce plan qui impose 10 jours de consolidation pour chaque kilo perdu en s'ouvrant progressivement à tout. Maintenant, je suis en phase 4, c'est à dire que je mange de tout sauf le jeudi où je contrôle. Je remercierai jamais assez l'auteur de ce livre.\n",
            "\n",
            "▁J ' ai ▁lu ▁ce ▁livre ▁car ▁dans ▁ma ▁ville , ▁tout ▁le ▁monde ▁s ' en ▁sert ▁et ▁le ▁commande . ▁C ' est ▁ma ▁pharmacie nne ▁qui ▁me ▁l ' a ▁conseil lé , ▁elle ▁a ▁tellement ▁mai gri ▁que ▁je ▁lui ▁ai ▁demandé ▁ce ▁qu ' elle ▁avait ▁fait ▁et ▁au ▁lieu ▁de ▁me ▁vendre ▁du ▁per lim pin pin ▁en ▁gél ules , ▁elle ▁m ' a ▁conseil lé ▁ce ▁livre ▁à ▁5 ▁euros . ▁Bien ▁sur , ▁il ▁faut ▁faire ▁un ▁effort ▁pour ▁perdre ▁25 ▁kilo s ▁mais ▁avec ▁le ▁livre , ▁j ' avais ▁un ▁compagno n ▁de ▁route . ▁L ' auteur ▁a ▁su ▁me ▁parler ▁simplement ▁avec ▁des ▁argument s ▁très ▁fort s ▁et ▁surtout ▁j ' ai ▁senti ▁qu ' il ▁connais s ait ▁bien ▁des ▁cas ▁comme ▁le ▁mi en . ▁Il ▁y ▁a ▁dans ▁son ▁texte ▁de ▁l ' expérience , ▁de ▁la ▁simplici té ▁et ▁de ▁la ▁com passion ▁pour ▁ceux ▁qui ▁comme ▁moi ▁viva it ▁avec ▁tout ▁ce ▁poids ▁qui ▁me ▁colla it ▁au ▁corps ▁sans ▁jamais ▁vouloir ▁partir . ▁Je ▁ne ▁crois ▁pas ▁qu ' il ▁existe ▁un ▁régime ▁miracle ▁qui ▁sur pass e ▁les ▁autres ▁mais ▁je ▁crois ▁vraiment ▁qu ' il ▁y ▁a ▁des ▁personnes ▁qui ▁sa vent ▁parler ▁aux ▁autres ▁et ▁faire ▁na it re ▁des ▁déc lic s . ▁Je ▁cro ya is ▁être ▁faible ▁mais ▁ce ▁livre ▁m ' a ▁rendu ▁forte , ▁je ▁l ' ai ▁tellement ▁anno té ▁que ▁j ' en ▁suis ▁à ▁mon ▁troisième . ▁Quand ▁on ▁est ▁très ▁grosse ▁comme ▁je ▁l ' ai ▁été , ▁les ▁non - gros ▁ne ▁vous ▁compren nent ▁pas ▁ou ▁ont ▁peur ▁de ▁vous ▁fro i sser ▁en ▁vous ▁en ▁parlant , ▁alors ▁ce ▁livre ▁a ▁été ▁comme ▁un ▁compagno n - journal . ▁Je ▁suis ▁pé di cure ▁et ▁je ▁l ' ai ▁conseil lé ▁à ▁tous ▁mes ▁clients ▁gros ▁dont ▁je ▁li s ▁la ▁so uff rance ▁sur ▁les ▁pieds ▁dé form és ▁et ▁ gon f lés . ▁je ▁rend s ▁aux ▁autres ▁le ▁service ▁que ▁m ' a ▁rendu ▁ma ▁pharmacie nne . ▁Je ▁le ▁conseille ▁à ▁tous ▁ceux ▁qui ▁so uff rent ▁car ▁après ▁avoir ▁mai gri ▁c ' est ▁un ▁tel ▁bonheur ▁que ▁j ' ai ▁accepté ▁de ▁passer ▁à ▁la ▁phase ▁3 ▁de ▁ce ▁plan ▁qui ▁ impose ▁10 ▁jours ▁de ▁consolida tion ▁pour ▁chaque ▁kilo ▁perdu ▁en ▁s ' ou v rant ▁progressive ment ▁à ▁tout . ▁Maintenant , ▁je ▁suis ▁en ▁phase ▁4 , ▁c ' est ▁à ▁dire ▁que ▁je ▁mange ▁de ▁tout ▁sauf ▁le ▁jeudi ▁où ▁je ▁contrôle . ▁Je ▁remercie rai ▁jamais ▁assez ▁l ' auteur ▁de ▁ce ▁livre .\n",
            "Ce livre explique techniquement et de façon très compréhensible, même pour des néophytes en physique des matériaux, comment et pourquoi la version officielle concernant l'effondrement des 3 tours du WTC ne tient tout simplement pas debout. Et c'est sans appel ! Il constitue un très bon argumentaire scientifique permettant de soutenir l'hypothèse la plus vraisemblable pour expliquer ce qui s'est réellement passé à New York, ce jour-là, l'hypothèse d'une démolition contrôlée !  A lire absolument !\n",
            "\n",
            "▁Ce ▁livre ▁explique ▁technique ment ▁et ▁de ▁façon ▁très ▁com pré hen sible , ▁même ▁pour ▁des ▁né o phy tes ▁en ▁physique ▁des ▁matériaux , ▁comment ▁et ▁pourquoi ▁la ▁version ▁officielle ▁concernant ▁l ' ef fond re ment ▁des ▁3 ▁tour s ▁du ▁W TC ▁ne ▁tient ▁tout ▁simplement ▁pas ▁de bou t . ▁Et ▁c ' est ▁sans ▁appel ▁! ▁Il ▁constitu e ▁un ▁très ▁bon ▁argument aire ▁scientifique ▁permettant ▁de ▁soutenir ▁l ' hy po th èse ▁la ▁plus ▁vrai sem bla ble ▁pour ▁explique r ▁ce ▁qui ▁s ' est ▁réellement ▁passé ▁à ▁New ▁York , ▁ce ▁jour - là , ▁l ' hy po th èse ▁d ' une ▁dé moli tion ▁cont rô lé e ▁! ▁A ▁lire ▁absolument ▁!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Binarizing the finetuning dataset</b>\n",
        "\n",
        "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
        "\n",
        "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>\n",
        "\n",
        "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
        "\n",
        "Use `!python libs/fairseq/fairseq_cli/preprocess.py --help` to get details about the arguments and visit the fairseq github repository for further help."
      ],
      "metadata": {
        "id": "iGNK19XuKBk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python libs/fairseq/fairseq_cli/preprocess.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yaDtn7fTyuL",
        "outputId": "e665c425-915b-4c01-9eeb-3e99d5f9d9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: preprocess.py [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
            "                     [--log-format {json,none,simple,tqdm}]\n",
            "                     [--log-file LOG_FILE] [--aim-repo AIM_REPO]\n",
            "                     [--aim-run-hash AIM_RUN_HASH]\n",
            "                     [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
            "                     [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
            "                     [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
            "                     [--memory-efficient-bf16] [--fp16]\n",
            "                     [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n",
            "                     [--fp16-init-scale FP16_INIT_SCALE]\n",
            "                     [--fp16-scale-window FP16_SCALE_WINDOW]\n",
            "                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
            "                     [--on-cpu-convert-precision]\n",
            "                     [--min-loss-scale MIN_LOSS_SCALE]\n",
            "                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
            "                     [--amp-batch-retries AMP_BATCH_RETRIES]\n",
            "                     [--amp-init-scale AMP_INIT_SCALE]\n",
            "                     [--amp-scale-window AMP_SCALE_WINDOW]\n",
            "                     [--user-dir USER_DIR]\n",
            "                     [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
            "                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
            "                     [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
            "                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
            "                     [--profile] [--reset-logging] [--suppress-crashes]\n",
            "                     [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
            "                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
            "                     [--tokenizer {moses,nltk,space}]\n",
            "                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
            "                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
            "                     [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
            "                     [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
            "                     [--task TASK] [-s SRC] [-t TARGET] [--trainpref FP]\n",
            "                     [--validpref FP] [--testpref FP] [--align-suffix FP]\n",
            "                     [--destdir DIR] [--thresholdtgt N] [--thresholdsrc N]\n",
            "                     [--tgtdict FP] [--srcdict FP] [--nwordstgt N]\n",
            "                     [--nwordssrc N] [--alignfile ALIGN]\n",
            "                     [--dataset-impl FORMAT] [--joined-dictionary]\n",
            "                     [--only-source] [--padding-factor N] [--workers N]\n",
            "                     [--dict-only]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --no-progress-bar     disable progress bar\n",
            "  --log-interval LOG_INTERVAL\n",
            "                        log progress every N batches (when progress bar is\n",
            "                        disabled)\n",
            "  --log-format {json,none,simple,tqdm}\n",
            "                        log format to use\n",
            "  --log-file LOG_FILE   log file to copy metrics to.\n",
            "  --aim-repo AIM_REPO   path to Aim repository\n",
            "  --aim-run-hash AIM_RUN_HASH\n",
            "                        Aim run hash. If skipped, creates or continues run\n",
            "                        based on save_dir\n",
            "  --tensorboard-logdir TENSORBOARD_LOGDIR\n",
            "                        path to save logs for tensorboard, should match\n",
            "                        --logdir of running tensorboard (default: no\n",
            "                        tensorboard logging)\n",
            "  --wandb-project WANDB_PROJECT\n",
            "                        Weights and Biases project name to use for logging\n",
            "  --azureml-logging     Log scalars to AzureML context\n",
            "  --seed SEED           pseudo random number generator seed\n",
            "  --cpu                 use CPU instead of CUDA\n",
            "  --tpu                 use TPU instead of CUDA\n",
            "  --bf16                use bfloat16; implies --tpu\n",
            "  --memory-efficient-bf16\n",
            "                        use a memory-efficient version of BF16 training;\n",
            "                        implies --bf16\n",
            "  --fp16                use FP16\n",
            "  --memory-efficient-fp16\n",
            "                        use a memory-efficient version of FP16 training;\n",
            "                        implies --fp16\n",
            "  --fp16-no-flatten-grads\n",
            "                        don't flatten FP16 grads tensor\n",
            "  --fp16-init-scale FP16_INIT_SCALE\n",
            "                        default FP16 loss scale\n",
            "  --fp16-scale-window FP16_SCALE_WINDOW\n",
            "                        number of updates before increasing loss scale\n",
            "  --fp16-scale-tolerance FP16_SCALE_TOLERANCE\n",
            "                        pct of updates that can overflow before decreasing the\n",
            "                        loss scale\n",
            "  --on-cpu-convert-precision\n",
            "                        if set, the floating point conversion to fp16/bf16\n",
            "                        runs on CPU. This reduces bus transfer time and GPU\n",
            "                        memory usage.\n",
            "  --min-loss-scale MIN_LOSS_SCALE\n",
            "                        minimum FP16/AMP loss scale, after which training is\n",
            "                        stopped\n",
            "  --threshold-loss-scale THRESHOLD_LOSS_SCALE\n",
            "                        threshold FP16 loss scale from below\n",
            "  --amp                 use automatic mixed precision\n",
            "  --amp-batch-retries AMP_BATCH_RETRIES\n",
            "                        number of retries of same batch after reducing loss\n",
            "                        scale with AMP\n",
            "  --amp-init-scale AMP_INIT_SCALE\n",
            "                        default AMP loss scale\n",
            "  --amp-scale-window AMP_SCALE_WINDOW\n",
            "                        number of updates before increasing AMP loss scale\n",
            "  --user-dir USER_DIR   path to a python module containing custom extensions\n",
            "                        (tasks and/or architectures)\n",
            "  --empty-cache-freq EMPTY_CACHE_FREQ\n",
            "                        how often to clear the PyTorch CUDA cache (0 to\n",
            "                        disable)\n",
            "  --all-gather-list-size ALL_GATHER_LIST_SIZE\n",
            "                        number of bytes reserved for gathering stats from\n",
            "                        workers\n",
            "  --model-parallel-size MODEL_PARALLEL_SIZE\n",
            "                        total number of GPUs to parallelize model over\n",
            "  --quantization-config-path QUANTIZATION_CONFIG_PATH\n",
            "                        path to quantization config file\n",
            "  --profile             enable autograd profiler emit_nvtx\n",
            "  --reset-logging       when using Hydra, reset the logging at the beginning\n",
            "                        of training\n",
            "  --suppress-crashes    suppress crashes when training with the hydra_train\n",
            "                        entry point so that the main method can return a value\n",
            "                        (useful for sweeps)\n",
            "  --use-plasma-view     Store indices and sizes in shared memory\n",
            "  --plasma-path PLASMA_PATH\n",
            "                        path to run plasma_store, defaults to /tmp/plasma.\n",
            "                        Paths outside /tmp tend to fail.\n",
            "  --criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}\n",
            "  --tokenizer {moses,nltk,space}\n",
            "  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}\n",
            "  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}\n",
            "  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}\n",
            "  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}\n",
            "  --task TASK           task\n",
            "  --dataset-impl FORMAT\n",
            "                        output dataset implementation\n",
            "\n",
            "Preprocessing:\n",
            "  -s SRC, --source-lang SRC\n",
            "                        source language\n",
            "  -t TARGET, --target-lang TARGET\n",
            "                        target language\n",
            "  --trainpref FP        train file prefix (also used to build dictionaries)\n",
            "  --validpref FP        comma separated, valid file prefixes (words missing\n",
            "                        from train set are replaced with <unk>)\n",
            "  --testpref FP         comma separated, test file prefixes (words missing\n",
            "                        from train set are replaced with <unk>)\n",
            "  --align-suffix FP     alignment file suffix\n",
            "  --destdir DIR         destination dir\n",
            "  --thresholdtgt N      map words appearing less than threshold times to\n",
            "                        unknown\n",
            "  --thresholdsrc N      map words appearing less than threshold times to\n",
            "                        unknown\n",
            "  --tgtdict FP          reuse given target dictionary\n",
            "  --srcdict FP          reuse given source dictionary\n",
            "  --nwordstgt N         number of target words to retain\n",
            "  --nwordssrc N         number of source words to retain\n",
            "  --alignfile ALIGN     an alignment file (optional)\n",
            "  --joined-dictionary   Generate joined dictionary\n",
            "  --only-source         Only process the source language\n",
            "  --padding-factor N    Pad dictionary size to be multiple of N\n",
            "  --workers N           number of parallel workers\n",
            "  --dict-only           if true, only builds a dictionary and then exits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = 'data/cls.books'\n",
        "input_dir = 'data/cls-books-bin/input0'\n",
        "label_dir = 'data/cls-books-bin/label'"
      ],
      "metadata": {
        "id": "69c4H3-JZal7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "              --only-source \\\n",
        "              --source-lang fr \\\n",
        "              --trainpref $TEXT/train.spm.review \\\n",
        "              --validpref $TEXT/valid.spm.review \\\n",
        "              --testpref $TEXT/test.spm.review \\\n",
        "              --srcdict models/RoBERTa_small_fr/dict.txt \\\n",
        "              --dict-only \\\n",
        "              --destdir $input_dir \\\n",
        "              --workers 8)\n",
        "              #fill me - binarize the tokenized reviews"
      ],
      "metadata": {
        "id": "jIF1wvWoFp4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8607196b-2175-48ce-aece-49acfe4262bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-19 10:18:42 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data/cls-books-bin/input0', dict_only=True, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='fr', srcdict='models/RoBERTa_small_fr/dict.txt', suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref='data/cls.books/test.spm.review', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='data/cls.books/train.spm.review', use_plasma_view=False, user_dir=None, validpref='data/cls.books/valid.spm.review', wandb_project=None, workers=8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "              --only-source \\\n",
        "              --source-lang fr \\\n",
        "              --trainpref $TEXT/train.label \\\n",
        "              --validpref $TEXT/valid.label \\\n",
        "              --testpref $TEXT/test.label \\\n",
        "              --srcdict models/RoBERTa_small_fr/dict.txt \\\n",
        "              --dict-only\\\n",
        "              --destdir $label_dir \\\n",
        "              --workers 8)#fill me - binarize the labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFdMKn5DWZAL",
        "outputId": "286ac641-14b3-4a72-b831-3fdb312d4335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-19 10:19:39 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data/cls-books-bin/label', dict_only=True, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='fr', srcdict='models/RoBERTa_small_fr/dict.txt', suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref='data/cls.books/test.label', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='data/cls.books/train.label', use_plasma_view=False, user_dir=None, validpref='data/cls.books/valid.label', wandb_project=None, workers=8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
        "\n",
        "In this section you will use <b>fairseq/fairseq_cli/train.py</b> python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2. \n",
        "\n",
        "Make sure to use the following hyper-parameters: $\\textit{batch size}=8, \\textit{max number of epochs}: 5, \\textit{optimizer}: Adam, \\textit{max learning rate}: 1e-05,  \\textit{warm up ratio}: 0.06, \\textit{learning rate scheduler}: linear$"
      ],
      "metadata": {
        "id": "2SSjBcQJnuSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "TASK= # fill me, sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr'\n",
        "DATA_PATH= # fill me \n",
        "MODEL_PATH= # fill me\n",
        "MAX_EPOCH= # fill me \n",
        "MAX_SENTENCES= # fill me, batch size\n",
        "MAX_UPDATE= # fill me, n_epochs * n_train_examples / total batch size \n",
        "LR= # fill me\n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score. \n",
        "METRIC = # fill me, use the accuracy metric\n",
        "NUM_CLASSES=#fill me, number of classes\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP = # fill me, warmup ratio=6% of the whole training"
      ],
      "metadata": {
        "id": "JV2112YPJEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric $METRIC \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $WARMUP \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)\n"
      ],
      "metadata": {
        "id": "6Mdznms-EYyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Random $RoBERTa_{small}^{fr}$ model training:</b>\n",
        "\n",
        "In this section you have to finetune a random checkpinf of the model $RoBERTa_{small}^{fr}$ using the same setting as before (<b>Hint:</b> an unexisted model path will not give you an error) "
      ],
      "metadata": {
        "id": "wi1U19Uunnse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "TASK= # fill me, sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr_random'\n",
        "DATA_PATH= # fill me \n",
        "MODEL_PATH= # fill me\n",
        "MAX_EPOCH= # fill me \n",
        "MAX_SENTENCES= # fill me, batch size\n",
        "MAX_UPDATE= # fill me, n_epochs * n_train_examples / total batch size \n",
        "LR= # fill me\n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score. \n",
        "METRIC = # fill me, use the accuracy metric\n",
        "NUM_CLASSES=#fill me, number of classes\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP = # fill me, warmup ratio=6% of the whole training"
      ],
      "metadata": {
        "id": "lSLWP6VUhUlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric $METRIC \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $WARMUP \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)"
      ],
      "metadata": {
        "id": "qtsCysc4hb42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Tensorboard Visualisation </B>\n",
        "\n",
        "In the this we will use tensorboard to visualize the training, validation and test accuracies. <b>Include and analyse in you report a screenshot of the test accuracy of the six models</b>."
      ],
      "metadata": {
        "id": "eHACXaPSLwu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir tensorboard_logs"
      ],
      "metadata": {
        "id": "pwVvJNExS2dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Part 2: HuggingFace's Transfromers</b>\n",
        "\n",
        "In this part of the lab, we will finetune a HuggingFace checkpoint of our $RoBERTa_{small}^{fr}$ on the CLS_Books dataset. Like in the first part we will start by downloading the HuggingFace checkpoint and <b>preparing a json format of the CLS_Books dataset</b> (Which is suitable for HuggingFace's checkpoints finetuning). Again, if you are using you personal computer, do not run the following cell and use its content to download the files on you computer, since - depending on you operating system - running this cell will produce errors. "
      ],
      "metadata": {
        "id": "PAR_P343MCKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd models\n",
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%21267607&authkey=APJub1wVzVLAoR8\" -O \"model_huggingface.zip\"\n",
        "!unzip model_huggingface.zip\n",
        "!rm model_huggingface.zip\n",
        "!rm -rf __MACOSX/\n",
        "\n",
        "%cd ../data\n",
        "!mkdir cls.books-json\n",
        "\n",
        "%cd .."
      ],
      "metadata": {
        "id": "v2ni-Bbql-CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Converting the CLS_Books dataset to json line files</b>\n",
        "\n",
        "Unlike Fairseq, you do not need to perform tokenization and binarization in Hugging Face transformer library. However, in order to use the implemented script in the transformers library, you need to convert your data to json line files (for each split: train, valid and test)\n",
        "\n",
        "for instance, each line inside you file will consist of one and one sample only, contaning the review (accessed by the key <i>sentence1</i> and its label, accessed by the key <i>label</i>. Below you can find an example from <i>valid.json</i> file.\n",
        "\n",
        "Note that these instructions are not valid for all kind of tasks. For other types of tasks (supported in Hugging face) you have to refer to their github for more details.<br>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "<i>\n",
        "{\"sentence1\":\"Seul ouvrage fran\\u00e7ais sur le th\\u00e8me Produits Structur\\u00e9s \\/ fonds \\u00e0 formule, il permet de fa\\u00e7on p\\u00e9dagogique d'appr\\u00e9hender parfaitement les m\\u00e9canismes financiers utilis\\u00e9s. Une r\\u00e9f\\u00e9rence pour ceux qui veulent comprendre les technicit\\u00e9s de base et les raisons de l'engouement des investisseurs sur ces actifs \\u00e0 hauteur de plusieurs milliards d'euros.\",\"label\":\"1\"}<br>\n",
        "{\"sentence1\":\"Livre tr\\u00e8s int\\u00e9ressant !  mais si comme moi vous cherchez des \\\"infos\\\" sur les techniques de sorties et autres \\\"modes d'emploi\\\", afin de vivre par vous m\\u00eame ce genre d'exp\\u00e9rience, c'est pas le bon livre.  \\u00e7a ne lui enl\\u00e8ve d'ailleurd rien \\u00e0 son int\\u00earet.\",\"label\":\"0\"}\n",
        "</i>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "c3M090L45oPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.review', 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "    with open('data/cls.books/'+split+'.label', 'r') as f:\n",
        "        labels = f.readlines()\n",
        "    with open('data/cls.books-json/'+split+'.json', 'w') as f:   \n",
        "        #fill the gap here to create train.json, valid.json and test.json\n"
      ],
      "metadata": {
        "id": "HZZFHEHFyv5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$ using the Transformers Library</b>\n",
        "\n",
        "In order to finrtune the model using HuggingFace, you to use the <b>run_glue.py</b> Python script located in the transformers library. For more details, refer to <a href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\" target=\"_blank\">the Huggingface/transformers repository on Github</a>. Make sure to use the same hyperparameter as in the first part of this lab."
      ],
      "metadata": {
        "id": "ICnN2FvnhTbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "MODEL='RoBERTa_small_fr_huggingface'\n",
        "MAX_SENTENCES= # fill me, batch size.\n",
        "LR=#fill me, learning rate\n",
        "MAX_EPOCH=#fill me\n",
        "NUM_CLASSES=#fill me\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0"
      ],
      "metadata": {
        "id": "h-BBIykNjH7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "    \n",
        "  )#fill me "
      ],
      "metadata": {
        "id": "lV2Zla33hK_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir checkpoints"
      ],
      "metadata": {
        "id": "d2UMHjatpFvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}